{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/skp454/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "UsageError: Line magic function `%matplotlib.inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "%matplotlib.inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = ['hindiMedium.csv', 'bareillyKiBarfi.csv', 'simran.csv', 'newton.csv', 'sanju.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bajrangiBhaijaan.csv', 'gabbarIsBack.csv', 'brothers.csv', 'hindiMedium.csv', 'dilwale.csv', 'khoobsurat.csv', 'bandBaajaBaraat.csv', 'dilDhadakneDo.csv', 'bareillyKiBarfi.csv', 'madrasCafe.csv', 'simran.csv', 'newton.csv', 'aurangzeb.csv', 'sanju.csv', 'bhoothnathReturns.csv']\n"
     ]
    }
   ],
   "source": [
    "## Find list of csv\n",
    "csvs = []\n",
    "for i in os.listdir('/scratch/skp454/AST/MainDir/data/'):\n",
    "    try:\n",
    "        if i.split('.')[1] == 'csv':\n",
    "            csvs.append(i)\n",
    "    except:\n",
    "        pass\n",
    "print(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract text and make dictionary\n",
    "words = {}\n",
    "sentences =[]\n",
    "for i in csvs:\n",
    "    file = pd.read_csv( '/scratch/skp454/AST/MainDir/data/'+i , header = None)\n",
    "    file[4] = i.split('.')[0] + file[4]\n",
    "    for j in file.index:\n",
    "        sentences.append(file.loc[j,3])\n",
    "        for word in file.loc[j,3].split(' '):\n",
    "            try:\n",
    "                words[word] += 1\n",
    "            except KeyError:\n",
    "                words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    avoid = ['has', 'was', 'as', 'us', 'pass', 'does', 'boss', 'less', 'discuss']\n",
    "    if word not in avoid:\n",
    "        if WordNetLemmatizer().lemmatize(word, 'n') is not word:\n",
    "            word = WordNetLemmatizer().lemmatize(word,'n')\n",
    "    word = WordNetLemmatizer().lemmatize(word,'v')\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up using NLTK to prepare words for word2vec embedding\n",
    "new_words = {}\n",
    "\n",
    "for word in words.keys():\n",
    "    count = words[word]\n",
    "    word = lemmatize(word)\n",
    "    try:\n",
    "        new_words[word] += count\n",
    "    except KeyError:\n",
    "        new_words[word] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.9672131147541"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_words.keys()))/len(list(words.keys())) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import word 2 vec embedding\n",
    "# embed = gensim.models.KeyedVectors.load_word2vec_format('/scratch/skp454/bigDataML/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.55946225439503\n"
     ]
    }
   ],
   "source": [
    "# w2v_words ={}\n",
    "# a = 0\n",
    "# for i in new_words:\n",
    "#     a+=1\n",
    "#     if i in embed.index2word:\n",
    "#         w2v_words[i] = new_words[i]\n",
    "#     if a % 50 == 0:\n",
    "#         clear_output()\n",
    "#         print(a*100/9670)\n",
    "# print(len(list(w2v_words.keys()))/len(list(words.keys())))\n",
    "# print(len(list(new_words.keys())))\n",
    "# print(sum(list(w2v_words.values()))/sum(list(words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words lost:  15515.0  % Lost:  9.44947590886\n",
      "Total Vocab lost:  8044  % Lost:  82.428115016\n",
      "Length of new dict:  1320\n",
      "Total words:  148674\n"
     ]
    }
   ],
   "source": [
    "# count loss if less occuring words are removed\n",
    "thres = 10\n",
    "\n",
    "values = list(new_words.values())\n",
    "values.sort()\n",
    "df = pd.DataFrame(pd.Series(values).value_counts())\n",
    "df[1] = df[0] * df.index\n",
    "df[2] = (np.cumsum(df[1])/sum(list(words.values())))\n",
    "df[3] = (np.cumsum(df[0])/sum(df[0]))\n",
    "print('Total words lost: ', str(df[df.index==thres][2][thres]*sum(list(words.values()))), ' % Lost: ',str(df[df.index==thres][2][thres]*100))\n",
    "print('Total Vocab lost: ', int(df[df.index==thres][3][thres]*len(list(words.keys()))), ' % Lost: ',df[df.index==thres][3][thres]*100)\n",
    "\n",
    "final_words = {}\n",
    "for word in new_words:\n",
    "    if new_words[word] > 10:\n",
    "        final_words[word] = new_words[word]\n",
    "print('Length of new dict: ', len(list(final_words.keys())))\n",
    "print('Total words: ', sum(list(final_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences lost: 519 % Sentences lost:  0.017918795746443863\n"
     ]
    }
   ],
   "source": [
    "new_sentences =[]\n",
    "for i in csvs:\n",
    "    file = pd.read_csv( '/scratch/skp454/AST/MainDir/data/'+i , header = None)\n",
    "    file[4] = i.split('.')[0] + file[4]\n",
    "    for j in file.index:\n",
    "        include = False\n",
    "        sent  = []\n",
    "        for word in file.loc[j,3].split(' '):\n",
    "            word = lemmatize(word)\n",
    "            if word in list(final_words.keys()):\n",
    "                include = True\n",
    "            else:\n",
    "                word = '_'\n",
    "            sent.append(word)\n",
    "        if include == True:\n",
    "            new_sentences.append(sent)\n",
    "print('Total Sentences lost:', len(sentences) - len(new_sentences),\\\n",
    "      '% Sentences lost: ', (len(sentences) - len(new_sentences))/len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save strength of words in dictionary\n",
    "outfile = open('/scratch/skp454/AST/MainDir/data/'+'dictionary_str.pkl','wb')\n",
    "pickle.dump(final_words ,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary mapping\n",
    "dictionary = {'<s>':0,'</s>':1,'_':2}\n",
    "a = 3\n",
    "for word in list(final_words.keys()):\n",
    "    dictionary[word] = a\n",
    "    a +=1\n",
    "del dictionary['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary\n",
    "outfile = open('/scratch/skp454/AST/MainDir/data/'+'dictionary.pkl','wb')\n",
    "pickle.dump(dictionary ,outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
