{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/skp454/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas.core.internals as pci\n",
    "import random\n",
    "import os\n",
    "import encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Audio Set\n",
    "\n",
    "1. Import all the audio pkls\n",
    "2. merge them together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory to pick files from\n",
    "dest = '/scratch/skp454/AST/MainDir/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for i in os.listdir(dest):\n",
    "    if 'Audio.pickle' in i:\n",
    "        files.append(i)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the pickle files\n",
    "pickles = {}\n",
    "for i in files:\n",
    "    with (open(dest+i, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                pickles.update(pickle.load(openfile, encoding = 'latin1')) \n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero pad all the audio fetaures\n",
    "lent = []\n",
    "#find max length\n",
    "# max_len = 0\n",
    "# for i in list(pickles.keys()):\n",
    "#     lent.append(pickles[i].shape[1])\n",
    "#     if pickles[i].shape[1] > max_len:\n",
    "#         max_len = pickles[i].shape[1]\n",
    "\n",
    "max_len = 1034        \n",
    "# zero pad\n",
    "for i in list(pickles.keys()):\n",
    "    if pickles[i].shape[1] > max_len:\n",
    "        del pickles[i]\n",
    "    else:\n",
    "        pickles[i] = np.concatenate((pickles[i],np.zeros([80,max_len-pickles[i].shape[1],3])), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Text Set\n",
    "\n",
    "1. Find relevant csvs\n",
    "2. for each line in csv make a dict of audio name: text\n",
    "3. make the listofall sentences and then encoder from it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open(dest+'dictionary.pkl', \"rb\")) as openfile:\n",
    "    dictionary = pickle.load(openfile, encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    avoid = ['has', 'was', 'as', 'us', 'pass', 'does', 'boss', 'less', 'discuss']\n",
    "    if word not in avoid:\n",
    "        if WordNetLemmatizer().lemmatize(word, 'n') is not word:\n",
    "            word = WordNetLemmatizer().lemmatize(word,'n')\n",
    "    word = WordNetLemmatizer().lemmatize(word,'v')\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, dictionary, max_size):\n",
    "    text = '<s> '+ text +' </s>'\n",
    "    outs = []\n",
    "    for word in text.split(' '):\n",
    "        word = word.strip()\n",
    "        if (word != \"\"):\n",
    "            word = lemmatize(word)\n",
    "            if word in list(dictionary.keys()):\n",
    "                outs.append(dictionary[word])\n",
    "                \n",
    "    if len(outs) < max_size + 2:\n",
    "        for i in range(max_size + 2 - len(outs)):\n",
    "            outs.append(dictionary['_'])\n",
    "    return np.array(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bajrangiBhaijaan.csv', 'gabbarIsBack.csv', 'brothers.csv', 'hindiMedium.csv', 'dilwale.csv', 'khoobsurat.csv', 'bandBaajaBaraat.csv', 'dilDhadakneDo.csv', 'bareillyKiBarfi.csv', 'madrasCafe.csv', 'simran.csv', 'newton.csv', 'aurangzeb.csv', 'sanju.csv', 'bhoothnathReturns.csv']\n"
     ]
    }
   ],
   "source": [
    "## Find list of csv\n",
    "csvs = []\n",
    "for i in os.listdir('/scratch/skp454/AST/MainDir/data/'):\n",
    "    try:\n",
    "        if i.split('.')[1] == 'csv':\n",
    "            csvs.append(i)\n",
    "    except:\n",
    "        pass\n",
    "print(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28964\n"
     ]
    }
   ],
   "source": [
    "## Extract text and make dictionary\n",
    "text = {}\n",
    "for i in csvs:\n",
    "    if i in ['hindiMedium.csv', 'bareillyKiBarfi.csv', 'simran.csv', 'newton.csv', 'sanju.csv']:\n",
    "        file = pd.read_csv( dest+i , header = None)\n",
    "        file[4] = i.split('.')[0] + file[4]\n",
    "        for j in file.index:\n",
    "            text[file.loc[j,4]] = file.loc[j,3]\n",
    "    else:\n",
    "        file = pd.read_csv( dest+i , header = None)\n",
    "        for j in file.index:\n",
    "            text[file.loc[j,4]] = file.loc[j,3]\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26835\n"
     ]
    }
   ],
   "source": [
    "# common id's\n",
    "common = 0\n",
    "uncommon = 0\n",
    "for i in list(pickles.keys()):\n",
    "    if i in list(text.keys()):\n",
    "        common+=1\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Train,Test & Dev Set\n",
    "1. make a master dict with keys as name of audiofile; value as a dict of { audio: audio features, text: encode text features}\n",
    "2. get there keys\n",
    "3. shuffle the list of keys\n",
    "4. Randomly split into 70:15:15 ratio.\n",
    "5. split the dictionaries based on these keys\n",
    "6. save the partioning list of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make master dict\n",
    "for i in list(pickles.keys()):\n",
    "    if i in list(text.keys()):\n",
    "        pickles[i] = {\"audio\": pickles[i], \"text\": encode(text[i], dictionary, 17) }\n",
    "    else:\n",
    "        del pickles[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26835"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len of master dict\n",
    "len(pickles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['simran4544.wav',\n",
       " 'simran5856.wav',\n",
       " 'simran5488.wav',\n",
       " 'simran5616.wav',\n",
       " 'simran4353.wav',\n",
       " 'simran5391.wav',\n",
       " 'simran4704.wav',\n",
       " 'simran5303.wav',\n",
       " 'simran5249.wav',\n",
       " 'simran5811.wav']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the keys from pickles\n",
    "keys = list(pickles.keys())\n",
    "\n",
    "# check keys\n",
    "print(len(keys))\n",
    "keys[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26835\n",
      "['simran5756.wav', 'aurangzeb12186.wav', 'sanju1093.wav', 'newton8435.wav', 'bhoothnathReturns10330.wav', 'dilDhadakneDo19516.wav', 'dilwale21622.wav', 'hindiMedium2683.wav', 'khoobsurat27089.wav', 'simran5176.wav']\n",
      "21468\n",
      "['simran5756.wav', 'aurangzeb12186.wav', 'sanju1093.wav', 'newton8435.wav', 'bhoothnathReturns10330.wav', 'dilDhadakneDo19516.wav', 'dilwale21622.wav', 'hindiMedium2683.wav', 'khoobsurat27089.wav', 'simran5176.wav']\n",
      "5367\n",
      "['dilwale21436.wav', 'hindiMedium2847.wav', 'dilDhadakneDo18342.wav', 'brothers17796.wav', 'madrasCafe28861.wav', 'bareillyKiBarfi7031.wav', 'bhoothnathReturns10779.wav', 'gabbarIsBack23394.wav', 'madrasCafe28732.wav', 'gabbarIsBack23174.wav']\n"
     ]
    }
   ],
   "source": [
    "# shuffle and split keys\n",
    "random.Random(4).shuffle(keys)\n",
    "\n",
    "# check keys\n",
    "print(len(keys))\n",
    "print(keys[0:10])\n",
    "\n",
    "#split\n",
    "train = keys[:int(len(keys)*0.8)]\n",
    "#dev = keys[int(len(keys)*0.7):int(len(keys)*0.85)]\n",
    "test = keys[int(len(keys)*0.8):]\n",
    "\n",
    "#check sets\n",
    "print(len(train))\n",
    "print(train[0:10])\n",
    "# print(len(dev))\n",
    "# print(dev[0:10])\n",
    "print(len(test))\n",
    "print(test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devSet = dict((k, pickles[k]) for k in dev)\n",
    "# outfile = open(dest+'dev_set.pkl','wb')\n",
    "# pickle.dump(devSet,outfile)\n",
    "# outfile.close()\n",
    "# del devSet, outfile\n",
    "# for k in dev:\n",
    "#     del pickles[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = dict((k, pickles[k]) for k in test)\n",
    "outfile = open(dest+'test_set.pkl','wb')\n",
    "pickle.dump(testSet,outfile)\n",
    "outfile.close()\n",
    "del testSet, outfile\n",
    "for k in test:\n",
    "    del pickles[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = dict((k, pickles[k]) for k in train)\n",
    "outfile = open(dest+'train_set.pkl','wb')\n",
    "pickle.dump(trainSet,outfile)\n",
    "outfile.close()\n",
    "del trainSet, outfile\n",
    "for k in train:\n",
    "    del pickles[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile = open(dest+'dictionary.pkl','wb')\n",
    "# pickle.dump(enc.dictionary ,outfile)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sample Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split keys\n",
    "random.Random(4).shuffle(train)\n",
    "s_train = train[:32]\n",
    "s_dev = train[32:48]\n",
    "\n",
    "# Save datasets\n",
    "devSet = dict((k, pickles[k]) for k in s_dev)\n",
    "outfile = open(dest+'s_dev_set.pkl','wb')\n",
    "pickle.dump(devSet,outfile)\n",
    "outfile.close()\n",
    "del devSet, outfile\n",
    "\n",
    "trainSet = dict((k, pickles[k]) for k in s_train)\n",
    "outfile = open(dest+'s_train_set.pkl','wb')\n",
    "pickle.dump(trainSet,outfile)\n",
    "outfile.close()\n",
    "del trainSet, outfile\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
